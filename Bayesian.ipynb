{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related data and code are also been found on https://github.com/Xuanstar42/BERN02_Exercise3.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response to Peer Review:\n",
    "1. Choice and justification of the model\n",
    "The chosen model fits the problem well and shows that the author understands the data and what\n",
    "needs to be estimated. Still, it would have been nice to include a short explanation of why this model\n",
    "was preferred over other possible ones - for example, what benefits it has compared to a simpler or\n",
    "non-hierarchical version.\n",
    "\n",
    "I have answered in this file. \n",
    "\n",
    "I chose a hierarchical Bayesian model because my project combines results from different studies. Hierarchical modeling allows partial pooling, so each study contributes to the overall estimate while still accounting for differences between them. This makes the evaluation of whether putting a sign increases towel reuse more robust than a non-hierarchical approach.\n",
    "\n",
    "2. Method for estimation and its implementation\n",
    "The estimation method works as intended and the implementation is clear. It's good that the author\n",
    "used an appropriate framework, but a short reflection on tuning choices or potential issues (like\n",
    "convergence or prior sensitivity) would make the section stronger.\n",
    "\n",
    "I met the problem of prior sensitivity. Please relook the part 3, I found the problem and fixed it in the part 4. \n",
    "\n",
    "3. Method for testing and reliability of the results\n",
    "The testing process looks reasonable, and the results seem consistent. However, it would improve\n",
    "clarity to explicitly write out the hypotheses being tested and briefly explain how the results support\n",
    "or reject them. Mentioning any limitations or uncertainty in the results would also add credibility.\n",
    "\n",
    "I have answered it in the intepretation part, with 5% or 6% uncertainty. \n",
    "\n",
    "4. Readability of the report\n",
    "The notebook is generally well-organized, with a logical order from data to results. A few small\n",
    "comments or summaries at the end of each section could help readers follow the reasoning more\n",
    "easily. Otherwise, the structure and explanations are quite clear and pleasant to read.\n",
    "\n",
    "I wrote before the paragraph why I use that code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "The goal of this analysis is to understand whether social norm messages influence the likelihood that people reuse towels in hotel rooms, compared to a control message.\n",
    "\n",
    "Each dataset entry summarizes the number of people who reused their towel (reuse) out of the total number of people observed (total) in a given study. There are several studies (for example, seven), each testing the same idea but in slightly different contexts.\n",
    "\n",
    "Because the outcome is a count of yes/no decisions (reuse or not), and the data come from multiple studies, a hierarchical binomial model is an appropriate statistical choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.signal import gaussian  \n",
    "except Exception:\n",
    "    from scipy.signal.windows import gaussian as _gaussian  \n",
    "    import scipy.signal as _sig\n",
    "    _sig.gaussian = _gaussian \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bambi as bmb\n",
    "import arviz as az\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reuse  total    group study\n",
      "0      74    211  control     1\n",
      "1     103    277  control     2\n",
      "2      77    135  control     3\n",
      "3      82    187  control     4\n",
      "4      21     25  control     5\n",
      "5     123    147  control     6\n",
      "6      28     30  control     7\n",
      "7      98    222   social     1\n",
      "8     587   1318   social     2\n",
      "9     406    655   social     3\n",
      "10    278    555   social     4\n",
      "11     21     24   social     5\n",
      "12    472    576   social     6\n",
      "13    101    132   social     7\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data_file = \"towelData.csv\"\n",
    "data = pd.read_csv(data_file, sep=';', encoding='latin1')\n",
    "count = data.iloc[:, -1] # get the last column with numbers or yes/no\n",
    "\n",
    "# count has the number of yes and no for control and social norm groups\n",
    "control_yes = count[::4].to_numpy() # every 4th starting from 0 - control group + yes\n",
    "control_no = count[2::4].to_numpy() # every 4th starting from 2 - control group + no\n",
    "control_total = np.array([y + n for y, n in zip(control_yes, control_no)])\n",
    "\n",
    "social_yes = count[1::4].to_numpy() # every 4th starting from 1 - social norm group + yes\n",
    "social_no = count[3::4].to_numpy() # every 4th starting from 3 - social norm group + no\n",
    "social_total = np.array([y + n for y, n in zip(social_yes, social_no)])\n",
    "\n",
    "study = np.arange(1,len(control_yes)+1) # 7 diferent studies\n",
    "\n",
    "control_data = pd.DataFrame({\"reuse\": control_yes, \"total\": control_total, \"group\": \"control\", \"study\": study})\n",
    "social_data = pd.DataFrame({ \"reuse\": social_yes, \"total\": social_total, \"group\": \"social\", \"study\": study})\n",
    "\n",
    "combined_data = pd.concat([control_data, social_data], ignore_index=True)\n",
    "\n",
    "# Convert data types - important for bambi that they are correctly set \n",
    "# can give errors otherwise\n",
    "combined_data['reuse'] = combined_data['reuse'].astype(int)\n",
    "combined_data['total'] = combined_data['total'].astype(int)\n",
    "combined_data['group'] = combined_data['group'].astype('category')\n",
    "combined_data['study'] = combined_data['study'].astype('category')\n",
    "print(combined_data.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Apply bmb Model\n",
    "1. I chose a hierarchical Bayesian model because my project combines results from different studies. Hierarchical modeling allows partial pooling, so each study contributes to the overall estimate while still accounting for differences between them. This makes the evaluation of whether putting a sign increases towel reuse more robust than a non-hierarchical approach.\n",
    "\n",
    "2. I chose the binomial model because the response variable is not a continuous measurement like temperature or height — it’s the number of successes out of a total number of trials.\n",
    "In this case:\n",
    "\n",
    "Success = a person reused their towel\n",
    "\n",
    "Trials = total number of participants in that condition\n",
    "\n",
    "3. Why use p(reuse, total)?\n",
    "\n",
    "This part defines the response variable for the binomial model:\n",
    "\n",
    "reuse = number of people who reused towels (successes)\n",
    "\n",
    "total = total number of people observed (trials)\n",
    "\n",
    "So I am  modeling the probability of reuse for each group and study.\n",
    "\n",
    "4. Why use 1 + group \n",
    "\n",
    "This specifies the fixed effects, meaning the general effects we expect across all studies.\n",
    "\n",
    "1 adds an intercept term. This represents the baseline log-odds of towel reuse in the control group.\n",
    "\n",
    "group adds a predictor for whether the message was “control” or “social.”\n",
    "It estimates how much the social norm message changes the reuse rate compared to the control message.\n",
    "\n",
    "5. Why use (1|study)\n",
    "\n",
    "This one introduces the random effect exist between the studies.\n",
    "Different studies might naturally have higher or lower reuse rates because of local conditions (different hotels, cultures, or sample sizes).\n",
    "By including (1|study), we give each study its own intercept, drawn from a shared distribution.\n",
    "\n",
    "This helps the model account for variation between studies rather than treating them as identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Formula: p(reuse, total) ~ 1 + group + (1|study)\n",
       "        Family: binomial\n",
       "          Link: p = logit\n",
       "  Observations: 14\n",
       "        Priors: \n",
       "    target = p\n",
       "        Common-level effects\n",
       "            Intercept ~ Normal(mu: 0.0, sigma: 3.5355)\n",
       "            group ~ Normal(mu: 0.0, sigma: 5.0)\n",
       "        \n",
       "        Group-level effects\n",
       "            1|study ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 3.5355))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hierarchical = bmb.Model(\"p(reuse, total) ~ 1 + group + (1|study)\", combined_data, family=\"binomial\")\n",
    "model_hierarchical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [Intercept, group, 1|study_sigma, 1|study_offset]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:05&lt;00:00 Sampling 4 chains, 22 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\n",
      "There were 22 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    }
   ],
   "source": [
    "idata_hierarchical = model_hierarchical.fit(random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was found that there are too many divergences. Therefore, we need to modify the prior.\n",
    "\n",
    "# 4. Prior Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [1|study_offset, 1|study_sigma, Intercept, group, p(reuse, total)]\n",
      "/var/folders/4s/2wy10by568x979b5g7g66m7c0000gn/T/ipykernel_11309/632727353.py:2: FutureWarning: extract_dataset has been deprecated, please use extract\n",
      "  prior = az.extract_dataset(idata_prior, group=\"prior_predictive\")[\"p(reuse, total)\"]\n"
     ]
    }
   ],
   "source": [
    "idata_prior = model_hierarchical.prior_predictive()\n",
    "prior = az.extract_dataset(idata_prior, group=\"prior_predictive\")[\"p(reuse, total)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Formula: p(reuse, total) ~ 1 + group + (1|study)\n",
       "        Family: binomial\n",
       "          Link: p = logit\n",
       "  Observations: 14\n",
       "        Priors: \n",
       "    target = p\n",
       "        Common-level effects\n",
       "            Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n",
       "            group ~ Normal(mu: 0.0, sigma: 5.0)\n",
       "        \n",
       "        Group-level effects\n",
       "            1|study ~ Normal(mu: 0.0, sigma: HalfNormal(sigma: 1.0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors = {\n",
    "    \"Intercept\": bmb.Prior(\"Normal\", mu=0, sigma=1),\n",
    "    \"1|study\": bmb.Prior(\"Normal\", mu=0, sigma=bmb.Prior(\"HalfNormal\", sigma=1))\n",
    "}\n",
    "model_hierarchical = bmb.Model(\"p(reuse, total) ~ 1 + group + (1|study)\", combined_data, family=\"binomial\", priors=priors)\n",
    "model_hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [Intercept, group, 1|study_sigma, 1|study_offset]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 00:05&lt;00:00 Sampling 4 chains, 2 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "idata_hierarchical = model_hierarchical.fit(random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 328kB\n",
      "Dimensions:            (chain: 4, draw: 1000, group_dim: 1, study__factor_dim: 7)\n",
      "Coordinates:\n",
      "  * chain              (chain) int64 32B 0 1 2 3\n",
      "  * draw               (draw) int64 8kB 0 1 2 3 4 5 ... 994 995 996 997 998 999\n",
      "  * group_dim          (group_dim) <U6 24B 'social'\n",
      "  * study__factor_dim  (study__factor_dim) <U1 28B '1' '2' '3' '4' '5' '6' '7'\n",
      "Data variables:\n",
      "    Intercept          (chain, draw) float64 32kB 0.1683 0.1897 ... 0.05734\n",
      "    group              (chain, draw, group_dim) float64 32kB 0.08793 ... 0.04418\n",
      "    1|study_sigma      (chain, draw) float64 32kB 1.052 1.137 ... 1.254 1.257\n",
      "    1|study            (chain, draw, study__factor_dim) float64 224kB -0.5754...\n",
      "Attributes:\n",
      "    created_at:                  2025-10-24T15:15:13.230916\n",
      "    arviz_version:               0.17.1\n",
      "    inference_library:           pymc\n",
      "    inference_library_version:   5.12.0\n",
      "    sampling_time:               5.54235315322876\n",
      "    tuning_steps:                1000\n",
      "    modeling_interface:          bambi\n",
      "    modeling_interface_version:  0.13.0\n"
     ]
    }
   ],
   "source": [
    "print(idata_hierarchical.posterior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Research Question**  \n",
    "Does the social norm intervention increase towel reuse compared to the control condition?\n",
    "\n",
    "**Hypotheses**  \n",
    "\n",
    "- **Null hypothesis (H₀):**  \n",
    "  The intervention has no effect.  \n",
    "  Mathematically: `group[social] = 0`.\n",
    "\n",
    "- **Alternative hypothesis (H₁):**  \n",
    "  The intervention is effective. The social norm group shows a higher reuse rate than the control group.  \n",
    "  Mathematically: `group[social] > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(group[social] > 0) = 0.99725\n"
     ]
    }
   ],
   "source": [
    "post = idata_hierarchical.posterior[\"group\"].sel(group_dim=\"social\").values\n",
    "p_gt0 = (post > 0).mean()\n",
    "print(\"P(group[social] > 0) =\", p_gt0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 0.99+, I can answer the question that the intervention is effective! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- The posterior probability that the intervention effect is greater than zero is nearly 1.0, \n",
    "  indicating very strong evidence for effectiveness.\n",
    "- The posterior mean of `group[social]` is about 0.21 (log-odds), with a 90% HDI of [0.09, 0.34].\n",
    "- This corresponds to approximately a 5–6 percentage point increase in towel reuse under the social norm condition compared to control.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
